{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142c2df5-2011-4aa1-8e70-9f91f22e0371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from data_utils import *\n",
    "from feature_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da3036f8-a126-443b-9ceb-a8be7a5d5ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global parameters:\n",
    "BASELINE = True # True for baseline model, false for advanced\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MULTILABEL = True\n",
    "\n",
    "if MULTILABEL:\n",
    "    task = 'argument-classification' # multilabel\n",
    "else:\n",
    "    task = 'argument-identification' # binary\n",
    "    \n",
    "if BASELINE:\n",
    "    model_type = 'baseline'\n",
    "else:\n",
    "    model_type = 'advanced'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79476835-931e-4dbb-acdb-7e14262a9c29",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "Importing data and applying transformation for conll format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88bd1f75-2921-483d-bdab-49f97378ad92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = '../Data/en_ewt-up-train.conllu'\n",
    "dev_path = '../Data/en_ewt-up-dev.conllu'\n",
    "test_path = '../Data/en_ewt-up-test.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8d5a4ea-13b9-4577-9092-8efad3c29243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = conll_transform(read_conll(train_path))\n",
    "dev_data = conll_transform(read_conll(dev_path))\n",
    "test_data = conll_transform(read_conll(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d849f54-9373-4770-aa3a-39d3dae68704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = extract_predicate_argument_feats(train_data)\n",
    "dev_data = extract_predicate_argument_feats(dev_data)\n",
    "test_data = extract_predicate_argument_feats(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51c3ea96-8bee-4c15-befd-36693d8f476e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get rid of unnecessary columns\n",
    "train_data.drop(['lemma', 'POS','morph_type','distance_head','dep_label','dep_rel'], axis=1, inplace=True)\n",
    "dev_data.drop(['lemma', 'POS','morph_type','distance_head','dep_label','dep_rel'], axis=1, inplace=True)\n",
    "test_data.drop(['lemma', 'POS','morph_type','distance_head','dep_label','dep_rel'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de865a46-a0c5-4c8e-a664-b1c08e86b2df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>Universal_POS</th>\n",
       "      <th>space</th>\n",
       "      <th>predicate</th>\n",
       "      <th>argument_type</th>\n",
       "      <th>is_token_predicate</th>\n",
       "      <th>is_token_argument</th>\n",
       "      <th>argument_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weblog-juancole.com_juancole_20051126063000_EN...</td>\n",
       "      <td>1</td>\n",
       "      <td>Al</td>\n",
       "      <td>NNP</td>\n",
       "      <td>SpaceAfter=No</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weblog-juancole.com_juancole_20051126063000_EN...</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>HYPH</td>\n",
       "      <td>SpaceAfter=No</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weblog-juancole.com_juancole_20051126063000_EN...</td>\n",
       "      <td>3</td>\n",
       "      <td>Zaman</td>\n",
       "      <td>NNP</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weblog-juancole.com_juancole_20051126063000_EN...</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weblog-juancole.com_juancole_20051126063000_EN...</td>\n",
       "      <td>5</td>\n",
       "      <td>American</td>\n",
       "      <td>JJ</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sent_id token_id     token  \\\n",
       "0  weblog-juancole.com_juancole_20051126063000_EN...        1        Al   \n",
       "1  weblog-juancole.com_juancole_20051126063000_EN...        2         -   \n",
       "2  weblog-juancole.com_juancole_20051126063000_EN...        3     Zaman   \n",
       "3  weblog-juancole.com_juancole_20051126063000_EN...        4         :   \n",
       "4  weblog-juancole.com_juancole_20051126063000_EN...        5  American   \n",
       "\n",
       "  Universal_POS          space predicate argument_type  is_token_predicate  \\\n",
       "0           NNP  SpaceAfter=No         _             _                   0   \n",
       "1          HYPH  SpaceAfter=No         _             _                   0   \n",
       "2           NNP              _         _             _                   0   \n",
       "3             :              _         _             _                   0   \n",
       "4            JJ              _         _             _                   0   \n",
       "\n",
       "   is_token_argument argument_label  \n",
       "0                  0              O  \n",
       "1                  0              O  \n",
       "2                  0              O  \n",
       "3                  0              O  \n",
       "4                  0              O  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "568c79ba-81b8-4d8e-b140-4053b5a363db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argument classification labels: {'ARGM-LVB', 'ARGM-PRR', 'ARG5', 'ARGM-NEG', 'ARGM-COM', 'ARGM-CAU', 'ARGM-PRD', 'ARGM-ADJ', 'ARGM-REC', 'ARGM-CXN', 'ARGM-GOL', 'ARGM-PRP', 'ARGA', 'ARGM-MNR', 'ARGM-ADV', 'ARG1-DSP', 'ARG4', 'ARG1', 'ARG3', 'ARG0', 'ARGM-MOD', 'ARGM-DIR', 'ARG2', 'O', 'ARGM-DIS', 'ARGM-LOC', 'ARGM-EXT', 'ARGM-TMP'}\n"
     ]
    }
   ],
   "source": [
    "if task == 'argument-identification':\n",
    "    label_list = set(train_data['is_token_argument'].tolist())\n",
    "    print(label_list)\n",
    "elif task == 'argument-classification':\n",
    "    label_list = set(train_data['argument_label'].tolist())\n",
    "    # for mapping str labels to int:\n",
    "    label_mapping = {}\n",
    "    for e, label in enumerate(label_list):\n",
    "        label_mapping.update({label: int(e)})\n",
    "    print(\"argument classification labels:\",label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0096154-3811-4eeb-a22b-e8e0f489a986",
   "metadata": {},
   "source": [
    "Following function represents each sentence data (tokens, predicate/argument labels, etc) to lists where each element is a list of tokens/labels/ids of a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f21dc95f-7562-468f-ab4e-6466116eb0a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_sentences(df: pd.DataFrame, baseline, multilabel):\n",
    "    \"\"\"\n",
    "    Extracts sentences, argument ids and labels from conll format\n",
    "    and puts it into lists, so that each element of list is list of sentence tokens/labels/ids.\n",
    "    Also applying either baseline or advanced model transformations for a sentence.\n",
    "    \n",
    "    params:\n",
    "    df: DataFrame of transformed conll with predicate argument features.\n",
    "    Baseline: True for baseline model else advanced model.\n",
    "    \n",
    "    Returns: \n",
    "    sentences: list of lists where each element is a token of given sentence. \n",
    "    arguments: list of lists of argument ids (binary).\n",
    "    arg_label: list of lists of argument labels (multilabel). \n",
    "    sentence_ids: list of sentence ids\n",
    "    \n",
    "    \"\"\"\n",
    "    model_type = 'baseline' if baseline else 'advanced'  \n",
    "    \n",
    "    sentences = []\n",
    "    arguments = [] # for argument ids (binary)\n",
    "    arg_label = [] # for argument labels (multilabel)\n",
    "    sentence_ids = []\n",
    "    \n",
    "    current_sent = []\n",
    "    current_sent_arguments = []\n",
    "    current_sent_arg_label = []\n",
    "    \n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if row['token_id'] == '1' and current_sent:\n",
    "            if model_type == 'baseline':\n",
    "                # add everything for baseline predicate mark at the end of sentence\n",
    "                current_sent.append('[SEP]')\n",
    "                current_sent.append(predicate_token)\n",
    "                current_sent_arguments.append(-100)\n",
    "                current_sent_arguments.append(-100)\n",
    "                current_sent_arg_label.append(-100)\n",
    "                current_sent_arg_label.append(-100)\n",
    "\n",
    "            \n",
    "            sentences.append(current_sent)\n",
    "            arguments.append(current_sent_arguments)\n",
    "            arg_label.append(current_sent_arg_label)\n",
    "            sentence_ids.append(current_id)\n",
    "            \n",
    "            current_sent = []\n",
    "            current_sent_arguments = []\n",
    "            current_sent_arg_label = []\n",
    "        \n",
    "        if model_type == 'baseline': \n",
    "            if row['is_token_predicate'] == 1:\n",
    "                predicate_token = row['token'] # saving predicate token for baseline model\n",
    "        \n",
    "            current_sent.append(row['token'])\n",
    "            current_sent_arguments.append(row['is_token_argument'])\n",
    "            \n",
    "            if multilabel:\n",
    "                current_sent_arg_label.append(label_mapping[row['argument_label']])\n",
    "            else:\n",
    "                current_sent_arg_label.append(row['argument_label'])                \n",
    "            current_id = row['sent_id']\n",
    "            \n",
    "        elif model_type == 'advanced':\n",
    "            \n",
    "            if row['is_token_predicate'] == 1:\n",
    "                # adding special token '[PREDICATE]' before predicate for advanced model\n",
    "                current_sent.append('[PREDICATE]')\n",
    "                current_sent.append(row['token'])\n",
    "                current_sent_arguments.append(-100)\n",
    "                current_sent_arguments.append(row['is_token_argument'])\n",
    "                current_sent_arg_label.append(-100)\n",
    "                if multilabel:\n",
    "                    current_sent_arg_label.append(label_mapping[row['argument_label']])\n",
    "                else:\n",
    "                    current_sent_arg_label.append(row['argument_label'])\n",
    "                \n",
    "            else:\n",
    "                current_sent.append(row['token'])\n",
    "                current_sent_arguments.append(row['is_token_argument'])\n",
    "                if multilabel:\n",
    "                    current_sent_arg_label.append(label_mapping[row['argument_label']])\n",
    "                else:\n",
    "                    current_sent_arg_label.append(row['argument_label'])\n",
    "                current_id = row['sent_id']\n",
    "           \n",
    "    return sentences, arguments, arg_label, sentence_ids  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3753fa64-ba8f-42ff-bfd6-8ee114a6539c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sents,  arguments, arg_label, sentence_ids = extract_sentences(train_data, baseline=BASELINE, multilabel=MULTILABEL)\n",
    "\n",
    "# Create a new DataFrame with the grouped data\n",
    "formatted_train = pd.DataFrame({\n",
    "    'sentence_id': sentence_ids,\n",
    "    'sentences': sents,\n",
    "    'is_argument': arguments, # binary - is_argument\n",
    "    'arg_labels': arg_label # multilabel\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6402c8bd-0187-49c8-9409-238248c87d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sents,  arguments, arg_label, sentence_ids = extract_sentences(dev_data, baseline=BASELINE, multilabel=MULTILABEL)\n",
    "\n",
    "# Create a new DataFrame with the grouped data\n",
    "formatted_dev = pd.DataFrame({\n",
    "    'sentence_id': sentence_ids,\n",
    "    'sentences': sents,\n",
    "    'is_argument': arguments, # binary - is_argument\n",
    "    'arg_labels': arg_label # multilabel\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74f9785b-8157-4907-935a-1c14cb3e05dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sents,  arguments, arg_label, sentence_ids = extract_sentences(test_data, baseline=BASELINE, multilabel=MULTILABEL)\n",
    "\n",
    "# Create a new DataFrame with the grouped data\n",
    "formatted_test = pd.DataFrame({\n",
    "    'sentence_id': sentence_ids,\n",
    "    'sentences': sents,\n",
    "    'is_argument': arguments, # binary - is_argument\n",
    "    'arg_labels': arg_label # multilabel\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b85671-9a75-4b71-86a0-148f9e2af7e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentences</th>\n",
       "      <th>is_argument</th>\n",
       "      <th>arg_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weblog-juancole.com_juancole_20051126063000_EN...</td>\n",
       "      <td>[Al, -, Zaman, :, American, forces, killed, Sh...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[23, 23, 23, 23, 23, 19, 23, 17, 23, 23, 23, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weblog-juancole.com_juancole_20051126063000_EN...</td>\n",
       "      <td>[[, This, killing, of, a, respected, cleric, w...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[23, 23, 23, 23, 23, 23, 17, 23, 23, 23, 23, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weblog-juancole.com_juancole_20051126063000_EN...</td>\n",
       "      <td>[DPA, :, Iraqi, authorities, announced, that, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[23, 23, 23, 19, 23, 23, 23, 23, 17, 23, 23, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weblog-juancole.com_juancole_20051126063000_EN...</td>\n",
       "      <td>[Two, of, them, were, being, run, by, 2, offic...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weblog-juancole.com_juancole_20051126063000_EN...</td>\n",
       "      <td>[The, MoI, in, Iraq, is, equivalent, to, the, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[23, 17, 23, 23, 23, 22, 23, 23, 23, 23, 23, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sentence_id  \\\n",
       "0  weblog-juancole.com_juancole_20051126063000_EN...   \n",
       "1  weblog-juancole.com_juancole_20051126063000_EN...   \n",
       "2  weblog-juancole.com_juancole_20051126063000_EN...   \n",
       "3  weblog-juancole.com_juancole_20051126063000_EN...   \n",
       "4  weblog-juancole.com_juancole_20051126063000_EN...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [Al, -, Zaman, :, American, forces, killed, Sh...   \n",
       "1  [[, This, killing, of, a, respected, cleric, w...   \n",
       "2  [DPA, :, Iraqi, authorities, announced, that, ...   \n",
       "3  [Two, of, them, were, being, run, by, 2, offic...   \n",
       "4  [The, MoI, in, Iraq, is, equivalent, to, the, ...   \n",
       "\n",
       "                                         is_argument  \\\n",
       "0  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          arg_labels  \n",
       "0  [23, 23, 23, 23, 23, 19, 23, 17, 23, 23, 23, 2...  \n",
       "1  [23, 23, 23, 23, 23, 23, 17, 23, 23, 23, 23, 2...  \n",
       "2  [23, 23, 23, 19, 23, 23, 23, 23, 17, 23, 23, 2...  \n",
       "3  [23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 2...  \n",
       "4  [23, 17, 23, 23, 23, 22, 23, 23, 23, 23, 23, 2...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d7fdbc0-adc6-46d1-b110-b4bfce8bfc61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "formatted_train.to_csv(f'../Data/transformers_formatted_train_{model_type}_{task}.csv', index=False)\n",
    "formatted_dev.to_csv(f'../Data/transformers_formatted_dev_{model_type}_{task}.csv', index=False)\n",
    "formatted_test.to_csv(f'../Data/transformers_formatted_test_{model_type}_{task}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b708338-ab02-44de-894f-adfd656e31ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "formatted_train = pd.read_csv(f'../Data/transformers_formatted_train_{model_type}_{task}.csv')\n",
    "formatted_dev = pd.read_csv(f'../Data/transformers_formatted_dev_{model_type}_{task}.csv')\n",
    "formatted_test = pd.read_csv(f'../Data/transformers_formatted_test_{model_type}_{task}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6914e0cf-4151-461d-a15a-2cefcb1f60e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the list columns are read as strings by pd.read_csv, thus converting it back to lists\n",
    "formatted_train = fix_lists(formatted_train)\n",
    "formatted_dev = fix_lists(formatted_dev)\n",
    "formatted_test = fix_lists(formatted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed132232-2ef2-4b1a-bb34-150e3dcc86f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# adding special token for advanced model:\n",
    "if BASELINE == False:   \n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': ['[PREDICATE]']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ddd0af-671e-4bfc-a4e2-6a458d3fcc3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(data, multilabel, label_all_tokens = True):\n",
    "    \"\"\"\n",
    "    Tokenizes the input examples and aligns argument labels and ids.\n",
    "\n",
    "    Parameters:\n",
    "    data: DataFrame containing tokens, sentence IDs, and argument labels/ids.\n",
    "    multilabel: True for argument classifcation else argument identification (binary).\n",
    "    label_all_tokens: bool for labeling all tokens.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of new examples with tokenized inputs and aligned labels.\n",
    "    \"\"\"\n",
    "    sentence_lists = data['sentences'].tolist()\n",
    "    sentence_ids = data['sentence_id'].tolist()\n",
    "    \n",
    "    # Tokenize sentences:\n",
    "    tokenized_inputs = tokenizer(sentence_lists, truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    aligned_examples = []\n",
    "    \n",
    "    for i, (is_arg, arg_label) in enumerate(zip(data['is_argument'], data['arg_labels'])):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        arg_ids = []\n",
    "        labels = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None: # set arg id and label to -100 for first and last special tokens\n",
    "                arg_ids.append(-100)\n",
    "                labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                arg_ids.append(is_arg[word_idx])\n",
    "                labels.append(arg_label[word_idx])\n",
    "            else:\n",
    "                arg_ids.append(is_arg[word_idx] if label_all_tokens else -100)\n",
    "                labels.append(arg_label[word_idx] if label_all_tokens else -100)\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        if multilabel:\n",
    "            aligned_examples.append({\n",
    "                'sentence_id': sentence_ids[i],\n",
    "                'sentence': sentence_lists[i],\n",
    "                'word_ids': word_ids,\n",
    "                'input_ids': tokenized_inputs['input_ids'][i],\n",
    "                'attention_mask': tokenized_inputs['attention_mask'][i],\n",
    "                'labels': labels,\n",
    "            })\n",
    "        else:\n",
    "            aligned_examples.append({\n",
    "                'sentence_id': sentence_ids[i],\n",
    "                'sentence': sentence_lists[i],\n",
    "                'word_ids': word_ids,\n",
    "                'input_ids': tokenized_inputs['input_ids'][i],\n",
    "                'attention_mask': tokenized_inputs['attention_mask'][i],\n",
    "                'labels': arg_ids,\n",
    "            })\n",
    "            \n",
    "    return aligned_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77acd3f4-beb3-440c-8f8d-22f59f27fe59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_train = tokenize_and_align_labels(formatted_train, MULTILABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3d5eac8-cb70-4228-bcf5-8b306bcc5499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dev = tokenize_and_align_labels(formatted_dev, MULTILABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f171dc77-d9e2-4ef2-8044-1218b3d50ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_test = tokenize_and_align_labels(formatted_test, MULTILABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f376e3c3-addc-4595-b377-0fad2ada3c82",
   "metadata": {},
   "source": [
    "number of examples in each set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea3fc875-e097-447c-90f8-e8d1f4b6dc0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41474\n",
      "5307\n",
      "5210\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_train))\n",
    "print(len(tokenized_dev))\n",
    "print(len(tokenized_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "482056f8-4ab8-477d-8666-28a43a9d566d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sentence_id', 'sentence', 'word_ids', 'input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e88acad-2f31-4462-ae6e-f37b51a23e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_id': 'weblog-juancole.com_juancole_20051126063000_ENG_20051126_063000-0001',\n",
       " 'sentence': ['Al',\n",
       "  '-',\n",
       "  'Zaman',\n",
       "  ':',\n",
       "  'American',\n",
       "  'forces',\n",
       "  'killed',\n",
       "  'Shaikh',\n",
       "  'Abdullah',\n",
       "  'al',\n",
       "  '-',\n",
       "  'Ani',\n",
       "  ',',\n",
       "  'the',\n",
       "  'preacher',\n",
       "  'at',\n",
       "  'the',\n",
       "  'mosque',\n",
       "  'in',\n",
       "  'the',\n",
       "  'town',\n",
       "  'of',\n",
       "  'Qaim',\n",
       "  ',',\n",
       "  'near',\n",
       "  'the',\n",
       "  'Syrian',\n",
       "  'border',\n",
       "  '.',\n",
       "  '[SEP]',\n",
       "  'killed'],\n",
       " 'word_ids': [None,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  22,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  None],\n",
       " 'input_ids': [101,\n",
       "  2632,\n",
       "  1011,\n",
       "  23564,\n",
       "  2386,\n",
       "  1024,\n",
       "  2137,\n",
       "  2749,\n",
       "  2730,\n",
       "  21146,\n",
       "  28209,\n",
       "  14093,\n",
       "  2632,\n",
       "  1011,\n",
       "  2019,\n",
       "  2072,\n",
       "  1010,\n",
       "  1996,\n",
       "  14512,\n",
       "  2012,\n",
       "  1996,\n",
       "  8806,\n",
       "  1999,\n",
       "  1996,\n",
       "  2237,\n",
       "  1997,\n",
       "  1053,\n",
       "  4886,\n",
       "  2213,\n",
       "  1010,\n",
       "  2379,\n",
       "  1996,\n",
       "  9042,\n",
       "  3675,\n",
       "  1012,\n",
       "  102,\n",
       "  2730,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  19,\n",
       "  23,\n",
       "  17,\n",
       "  17,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  25,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  23,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d9a1a-9e24-4016-9782-52ff532ffb1e",
   "metadata": {},
   "source": [
    "# Model setup and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7fec4d3-255f-4f7c-9181-500daba12c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 22:45:35.026430: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-03 22:45:35.873605: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-03-03 22:45:35.873738: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2024-03-03 22:45:35.873750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(label_list))\n",
    "# making sure that special token is added:\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "767db67c-7345-4967-a13b-36441df19974",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"{MODEL_NAME}-finetuned-{model_type}-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c570310-28b6-4364-b117-80d81d8b2474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac224278-0d87-4ac6-ad1c-49f06a27967e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "839a5be0-6b61-4482-9baa-0117acfc0791",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9079' max='12970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9079/12970 36:01 < 15:26, 4.20 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.269063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.233400</td>\n",
       "      <td>0.252631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.218100</td>\n",
       "      <td>0.247780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.210100</td>\n",
       "      <td>0.245847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.248094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.197500</td>\n",
       "      <td>0.248908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.193000</td>\n",
       "      <td>0.250099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory distilbert-base-uncased-finetuned-baseline-argument-classification/checkpoint-1297 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory distilbert-base-uncased-finetuned-baseline-argument-classification/checkpoint-2594 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory distilbert-base-uncased-finetuned-baseline-argument-classification/checkpoint-3891 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory distilbert-base-uncased-finetuned-baseline-argument-classification/checkpoint-5188 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory distilbert-base-uncased-finetuned-baseline-argument-classification/checkpoint-6485 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory distilbert-base-uncased-finetuned-baseline-argument-classification/checkpoint-7782 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory distilbert-base-uncased-finetuned-baseline-argument-classification/checkpoint-9079 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9079, training_loss=0.2221136002803718, metrics={'train_runtime': 2162.6311, 'train_samples_per_second': 191.776, 'train_steps_per_second': 5.997, 'total_flos': 6076192541468400.0, 'train_loss': 0.2221136002803718, 'epoch': 7.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e8cee-d38b-469b-90dd-21bc0c979012",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing model\n",
    "getting predictions on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e28b38a1-2608-4cbe-82eb-c9ad104cd312",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_predictions = trainer.predict(tokenized_test)\n",
    "test_preds = test_predictions.predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641bb1b6-1999-4a45-affd-5976ee6ffc75",
   "metadata": {},
   "source": [
    "Aggregating subtoken level logtis with tokens in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a542abfc-9232-4cc7-bda0-5841c953846b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aggregatted_test_preds = aggregate_subtoken_logits(tokenized_test, test_preds)\n",
    "aggregatted_test_preds = [np.argmax(pred, axis=1) for pred in aggregatted_test_preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f37a9b-c8a5-438f-9c44-4fc7f52f09a7",
   "metadata": {},
   "source": [
    "Aligns original labels/ids with their corresponding word-level predictions in tokenized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc595488-9b16-4b7d-bbb5-a83ac419fa83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aligned_true_test_labels = align_labels_with_predictions(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3da3e2ae-2a1c-4e62-8f8a-2640995247f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 23, 17, 23, 23, 22, 23, -100, -100]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_true_test_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0019983-d3e0-4656-9c8a-8c2d21e27c73",
   "metadata": {
    "tags": []
   },
   "source": [
    "remove special token indexes from predictions and gold labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c03b12ab-5c9f-49ae-859b-dac11a34a940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if MULTILABEL:\n",
    "    preds, true_labels = remove_special_token_indexes(aggregatted_test_preds, aligned_true_test_labels, \n",
    "                                                 label_list=list(label_mapping.values()))\n",
    "else:\n",
    "    preds, true_labels = remove_special_token_indexes(aggregatted_test_preds, aligned_true_test_labels, \n",
    "                                                 label_list=list(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2175670-81d1-4740-9903-511c1d583926",
   "metadata": {
    "tags": []
   },
   "source": [
    "computing classification metrics on test set predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f02a9244-f123-494d-8dbc-3a690d650f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_classification_metrics(preds, true_labels, multilabel):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, f1 score, and macro average metrics for classification results.\n",
    "    \n",
    "    Parameters:\n",
    "    preds: List of list of predictions from token classification\n",
    "    true_labels: List of list of true labels from token classification\n",
    "    multilabel: true for argument classification, else argument identification.\n",
    "    \n",
    "    return: \n",
    "    Dictionary with precision, recall, f1 score for each class and macro averages\n",
    "    \"\"\"\n",
    "    # Flatten the predictions and true labels lists\n",
    "    preds_flat = [p for sublist in preds for p in sublist]\n",
    "    true_flat = [t for sublist in true_labels for t in sublist]\n",
    "    \n",
    "    # Extract unique classes\n",
    "    if multilabel:\n",
    "        classes = list(label_mapping.values())\n",
    "    else:\n",
    "        classes = sorted(set(true_flat))\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score for each class\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_flat, preds_flat, labels=classes)\n",
    "    \n",
    "    # Calculate macro averages\n",
    "    precision_macro = np.mean(precision)\n",
    "    recall_macro = np.mean(recall)\n",
    "    f1_macro = np.mean(f1)\n",
    "    \n",
    "    # Create a dictionary to store the metrics\n",
    "    metrics = {\n",
    "        'classes': list(label_list) if multilabel else list(classes),\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'macro': {\n",
    "            'precision_macro': precision_macro,\n",
    "            'recall_macro': recall_macro,\n",
    "            'f1_macro': f1_macro\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "becc05c2-a72c-4572-bddf-550d8a25d3cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "results = calculate_classification_metrics(preds, true_labels, MULTILABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0df58239-1963-41ed-9d90-b3844ce1408c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': ['ARGM-LVB',\n",
       "  'ARGM-PRR',\n",
       "  'ARG5',\n",
       "  'ARGM-NEG',\n",
       "  'ARGM-COM',\n",
       "  'ARGM-CAU',\n",
       "  'ARGM-PRD',\n",
       "  'ARGM-ADJ',\n",
       "  'ARGM-REC',\n",
       "  'ARGM-CXN',\n",
       "  'ARGM-GOL',\n",
       "  'ARGM-PRP',\n",
       "  'ARGA',\n",
       "  'ARGM-MNR',\n",
       "  'ARGM-ADV',\n",
       "  'ARG1-DSP',\n",
       "  'ARG4',\n",
       "  'ARG1',\n",
       "  'ARG3',\n",
       "  'ARG0',\n",
       "  'ARGM-MOD',\n",
       "  'ARGM-DIR',\n",
       "  'ARG2',\n",
       "  'O',\n",
       "  'ARGM-DIS',\n",
       "  'ARGM-LOC',\n",
       "  'ARGM-EXT',\n",
       "  'ARGM-TMP'],\n",
       " 'precision': array([0.        , 0.40909091, 0.        , 0.69565217, 0.        ,\n",
       "        0.35714286, 0.28571429, 0.68181818, 0.        , 0.        ,\n",
       "        0.        , 0.33333333, 0.        , 0.46153846, 0.60759494,\n",
       "        0.        , 0.5       , 0.65936473, 0.52173913, 0.61389961,\n",
       "        0.63362069, 0.33333333, 0.55279503, 0.92759418, 0.6       ,\n",
       "        0.69444444, 0.73913043, 0.67164179]),\n",
       " 'recall': array([0.        , 0.26470588, 0.        , 0.15238095, 0.        ,\n",
       "        0.11363636, 0.04545455, 0.20737327, 0.        , 0.        ,\n",
       "        0.        , 0.05333333, 0.        , 0.04225352, 0.10041841,\n",
       "        0.        , 0.125     , 0.19092927, 0.16666667, 0.19053325,\n",
       "        0.34507042, 0.02222222, 0.16137806, 0.99120411, 0.38547486,\n",
       "        0.12690355, 0.16190476, 0.1734104 ]),\n",
       " 'f1': array([0.        , 0.32142857, 0.        , 0.25      , 0.        ,\n",
       "        0.17241379, 0.07843137, 0.3180212 , 0.        , 0.        ,\n",
       "        0.        , 0.09195402, 0.        , 0.07741935, 0.17235189,\n",
       "        0.        , 0.2       , 0.29611412, 0.25263158, 0.29080933,\n",
       "        0.44680851, 0.04166667, 0.24982456, 0.95834478, 0.46938776,\n",
       "        0.21459227, 0.265625  , 0.27565084]),\n",
       " 'macro': {'precision_macro': 0.40283744697126284,\n",
       "  'recall_macro': 0.14358049525661215,\n",
       "  'f1_macro': 0.19440984336002426}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd0c309a-87ac-483b-ac74-6f9863bb162d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_dict_to_json(results, f'../Results/{MODEL_NAME}-{model_type}-{task}results-final.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf151a-f63f-47cf-8f71-391f5232ce52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
