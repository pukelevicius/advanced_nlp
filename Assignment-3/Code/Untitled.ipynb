{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142c2df5-2011-4aa1-8e70-9f91f22e0371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79476835-931e-4dbb-acdb-7e14262a9c29",
   "metadata": {},
   "source": [
    "Importing data and applying transformation for conll format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88bd1f75-2921-483d-bdab-49f97378ad92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = '../Data/en_ewt-up-train.conllu'\n",
    "dev_path = '../Data/en_ewt-up-dev.conllu'\n",
    "test_path = '../Data/en_ewt-up-test.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d5a4ea-13b9-4577-9092-8efad3c29243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = conll_transform(read_conll(train_path))\n",
    "dev_data = conll_transform(read_conll(dev_path))\n",
    "test_data = conll_transform(read_conll(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e582a1f7-5ff6-4a58-addb-bb69251bac2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_predicate_argument_feats(df):\n",
    "    \"\"\"\n",
    "    Fuction to extract argument and predicate features from transformed\n",
    "    conll format data.\n",
    "    \n",
    "    params:\n",
    "    df: Dataframe of transformed conll data\n",
    "    \"\"\"\n",
    "    # feature to indicate if the token is a predicate; maybe redundant\n",
    "    df['is_token_predicate'] = (df['predicate'] != '_').astype(int)\n",
    "    # feature for classification task 1: argument identification\n",
    "    df['is_token_argument'] = (df['argument_type'].str.startswith('ARG')).astype(int)\n",
    "    # feature for classification task 2: argument classification\n",
    "    df['argument_label'] = df['argument_type'].apply(lambda x: x if x.startswith('ARG') else 'O')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d849f54-9373-4770-aa3a-39d3dae68704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = extract_predicate_argument_feats(train_data)\n",
    "dev_data = extract_predicate_argument_feats(dev_data)\n",
    "test_data = extract_predicate_argument_feats(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51c3ea96-8bee-4c15-befd-36693d8f476e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get rid of unnecessary columns\n",
    "train_data.drop(['lemma', 'POS','morph_type','distance_head','dep_label','dep_rel'], axis=1, inplace=True)\n",
    "dev_data.drop(['lemma', 'POS','morph_type','distance_head','dep_label','dep_rel'], axis=1, inplace=True)\n",
    "test_data.drop(['lemma', 'POS','morph_type','distance_head','dep_label','dep_rel'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0096154-3811-4eeb-a22b-e8e0f489a986",
   "metadata": {},
   "source": [
    "Following function represents each sentence data (tokens, predicate/argument labels, etc) to lists, therefore,\n",
    "info of each distinct sentence will be stored in designated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6255296e-444f-4a45-813f-15091ec44b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent setence in a list:\n",
    "def extract_sentences(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    extracts sentences from \n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    predicates = []\n",
    "    arguments = []\n",
    "    arg_label = []\n",
    "    sentence_ids = []\n",
    "    \n",
    "    current_sent = []\n",
    "    current_sent_predicates = []\n",
    "    current_sent_arguments = []\n",
    "    current_sent_arg_label = []\n",
    "    \n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if row['token_id'] == '1' and current_sent:\n",
    "            # add everything for baseline predicate mark at the end of sentence\n",
    "            current_sent.append('[SEP]')\n",
    "            current_sent.append(predicate_token)\n",
    "            current_sent_predicates.append(0)\n",
    "            current_sent_predicates.append(1)\n",
    "            current_sent_arguments.append(-100)\n",
    "            current_sent_arguments.append(-100)\n",
    "            current_sent_arg_label.append(-100)\n",
    "            current_sent_arg_label.append(-100)\n",
    "            \n",
    "            \n",
    "            sentences.append(current_sent)\n",
    "            predicates.append(current_sent_predicates)\n",
    "            arguments.append(current_sent_arguments)\n",
    "            arg_label.append(current_sent_arg_label)\n",
    "            sentence_ids.append(current_id)\n",
    "            \n",
    "            current_sent = []\n",
    "            current_sent_predicates = []\n",
    "            current_sent_arguments = []\n",
    "            current_sent_arg_label = []\n",
    "        \n",
    "        if row['is_token_predicate'] == 1:\n",
    "            predicate_token = row['token']\n",
    "        \n",
    "        current_sent.append(row['token'])\n",
    "        current_sent_predicates.append(row['is_token_predicate'])\n",
    "        current_sent_arguments.append(row['is_token_argument'])\n",
    "        current_sent_arg_label.append(row['argument_label'])\n",
    "        current_id = row['sent_id']\n",
    "    \n",
    "           \n",
    "    return sentences, predicates, arguments, arg_label, sentence_ids  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3753fa64-ba8f-42ff-bfd6-8ee114a6539c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sents, predicates, arguments, arg_label, sentence_ids = extract_sentences(train_data)\n",
    "\n",
    "# Create a new DataFrame with the grouped data\n",
    "formatted_train = pd.DataFrame({\n",
    "    'sentence_id': sentence_ids,\n",
    "    'sentences': sents,\n",
    "    'is_predicate': predicates, # binary - is_predicate\n",
    "    'is_argument': arguments, # binary - is_argument\n",
    "    'arg_labels': arg_label # multilabel\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6402c8bd-0187-49c8-9409-238248c87d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sents, predicates, arguments, arg_label, sentence_ids = extract_sentences(dev_data)\n",
    "\n",
    "# Create a new DataFrame with the grouped data\n",
    "formatted_dev = pd.DataFrame({\n",
    "    'sentence_id': sentence_ids,\n",
    "    'sentences': sents,\n",
    "    'is_predicate': predicates, # binary - is_predicate\n",
    "    'is_argument': arguments, # binary - is_argument\n",
    "    'arg_labels': arg_label # multilabel\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74f9785b-8157-4907-935a-1c14cb3e05dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sents, predicates, arguments, arg_label, sentence_ids = extract_sentences(test_data)\n",
    "\n",
    "# Create a new DataFrame with the grouped data\n",
    "formatted_test = pd.DataFrame({\n",
    "    'sentence_id': sentence_ids,\n",
    "    'sentences': sents,\n",
    "    'is_predicate': predicates, # binary - is_predicate\n",
    "    'is_argument': arguments, # binary - is_argument\n",
    "    'arg_labels': arg_label # multilabel\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b85671-9a75-4b71-86a0-148f9e2af7e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentences</th>\n",
       "      <th>is_predicate</th>\n",
       "      <th>is_argument</th>\n",
       "      <th>arg_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weblog-juancole.com_juancole_20051126063000_EN...</td>\n",
       "      <td>[Al, -, Zaman, :, American, forces, killed, Sh...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[O, O, O, O, O, ARG0, O, ARG1, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weblog-juancole.com_juancole_20051126063000_EN...</td>\n",
       "      <td>[[, This, killing, of, a, respected, cleric, w...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[O, O, O, O, O, O, ARG1, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weblog-juancole.com_juancole_20051126063000_EN...</td>\n",
       "      <td>[DPA, :, Iraqi, authorities, announced, that, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[O, O, O, ARG0, O, O, O, O, ARG1, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weblog-juancole.com_juancole_20051126063000_EN...</td>\n",
       "      <td>[Two, of, them, were, being, run, by, 2, offic...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weblog-juancole.com_juancole_20051126063000_EN...</td>\n",
       "      <td>[The, MoI, in, Iraq, is, equivalent, to, the, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[O, ARG1, O, O, O, ARG2, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sentence_id  \\\n",
       "0  weblog-juancole.com_juancole_20051126063000_EN...   \n",
       "1  weblog-juancole.com_juancole_20051126063000_EN...   \n",
       "2  weblog-juancole.com_juancole_20051126063000_EN...   \n",
       "3  weblog-juancole.com_juancole_20051126063000_EN...   \n",
       "4  weblog-juancole.com_juancole_20051126063000_EN...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [Al, -, Zaman, :, American, forces, killed, Sh...   \n",
       "1  [[, This, killing, of, a, respected, cleric, w...   \n",
       "2  [DPA, :, Iraqi, authorities, announced, that, ...   \n",
       "3  [Two, of, them, were, being, run, by, 2, offic...   \n",
       "4  [The, MoI, in, Iraq, is, equivalent, to, the, ...   \n",
       "\n",
       "                                        is_predicate  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, ...   \n",
       "3  [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "\n",
       "                                         is_argument  \\\n",
       "0  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          arg_labels  \n",
       "0  [O, O, O, O, O, ARG0, O, ARG1, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, ARG1, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, ARG0, O, O, O, O, ARG1, O, O, O, O, ...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4  [O, ARG1, O, O, O, ARG2, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70510176-b651-4717-83f5-625271d2906c",
   "metadata": {},
   "source": [
    "Saving processed to csv because running all cells sequentially exhausts all of the system ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d7fdbc0-adc6-46d1-b110-b4bfce8bfc61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "formatted_train.to_csv('../Data/transformers_formatted_train.csv', index=False)\n",
    "formatted_dev.to_csv('../Data/transformers_formatted_dev.csv', index=False)\n",
    "formatted_test.to_csv('../Data/transformers_formatted_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b708338-ab02-44de-894f-adfd656e31ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "formatted_train = pd.read_csv('../Data/transformers_formatted_train.csv')\n",
    "formatted_dev = pd.read_csv('../Data/transformers_formatted_dev.csv')\n",
    "formatted_test = pd.read_csv('../Data/transformers_formatted_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed132232-2ef2-4b1a-bb34-150e3dcc86f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f04fbae-8422-44e6-bdd5-1b8846657901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(data):\n",
    "    \"\"\"\n",
    "    Tokenizes the input examples and aligns argument labels and ids.\n",
    "\n",
    "    Parameters:\n",
    "    data: DataFrame containing tokens, sentence IDs, and argument labels/ids.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of new examples with tokenized inputs and aligned labels.\n",
    "    \"\"\"\n",
    "    sentence_lists = data['sentences'].tolist()\n",
    "    sentence_ids = data['sentence_id'].tolist()\n",
    "    \n",
    "    # Tokenize sentences:\n",
    "    tokenized_inputs = tokenizer(sentence_lists, truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    aligned_examples = []\n",
    "    \n",
    "    for i, (is_arg, arg_label) in enumerate(zip(data['is_argument'], data['arg_labels'])):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        arg_ids = []\n",
    "        labels = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                arg_ids.append(-100)\n",
    "                labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                arg_ids.append(is_arg[word_idx])\n",
    "                labels.append(arg_label[word_idx])\n",
    "            else:\n",
    "                arg_ids.append(is_arg[word_idx])\n",
    "                labels.append(arg_label[word_idx])\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "        aligned_examples.append({\n",
    "            'sentence_id': sentence_ids[i],\n",
    "            'sentence': sentence_lists[i],\n",
    "            'word_ids': word_ids,\n",
    "            'is_argument': arg_ids,\n",
    "            'argument_labels': labels,\n",
    "        })\n",
    "    return aligned_examples\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77acd3f4-beb3-440c-8f8d-22f59f27fe59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_train \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_and_align_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatted_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mtokenize_and_align_labels\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     17\u001b[0m aligned_examples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (is_arg, arg_label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_argument\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marg_labels\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[0;32m---> 20\u001b[0m     word_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     previous_word_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     arg_ids \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:384\u001b[0m, in \u001b[0;36mBatchEncoding.word_ids\u001b[0;34m(self, batch_index)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings:\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m     )\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encodings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mword_ids\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_align_labels(formatted_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e00b5f8-c80f-4710-9d97-e74d8271aca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5eac8-cb70-4228-bcf5-8b306bcc5499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dev = tokenize_and_align_labels(formatted_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171dc77-d9e2-4ef2-8044-1218b3d50ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_test = tokenize_and_align_labels(formatted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3fc875-e097-447c-90f8-e8d1f4b6dc0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(tokenized_train))\n",
    "print(len(tokenized_dev))\n",
    "print(len(tokenized_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e0590d-de38-4894-8c10-31bc5beb3daa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fec4d3-255f-4f7c-9181-500daba12c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767db67c-7345-4967-a13b-36441df19974",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-negation-scope\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy=\"epoch\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
